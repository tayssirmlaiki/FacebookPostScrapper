{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "from urllib.parse import urlparse, unquote\n",
    "from urllib.parse import parse_qs\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacebookPostsScraper:\n",
    "\n",
    "    # We need the email and password to access Facebook, and optionally the text in the Url that identifies the \"view full post\".\n",
    "    def __init__(self, email, password, post_url_text='Full Story'):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.headers = {  # This is the important part: Nokia C3 User Agent\n",
    "            'User-Agent': 'NokiaC3-00/5.0 (07.20) Profile/MIDP-2.1 Configuration/CLDC-1.1 Mozilla/5.0 AppleWebKit/420+ (KHTML, like Gecko) Safari/420+'\n",
    "        }\n",
    "        self.session = requests.session()  # Create the session for the next requests\n",
    "        self.cookies_path = 'session_facebook.cki'  # Give a name to store the session in a cookie file.\n",
    "\n",
    "        # At certain point, we need find the text in the Url to point the url post, in my case, my Facebook is in\n",
    "        # English, this is why it says 'Full Story', so, you need to change this for your language.\n",
    "        # Some translations:\n",
    "        # - English: 'Full Story'\n",
    "        # - Spanish: 'Historia completa'\n",
    "        self.post_url_text = post_url_text\n",
    "\n",
    "        # Evaluate if NOT exists a cookie file, if NOT exists the we make the Login request to Facebook,\n",
    "        # else we just load the current cookie to maintain the older session.\n",
    "        if self.new_session():\n",
    "            self.login()\n",
    "\n",
    "        self.posts = []  # Store the scraped posts\n",
    "\n",
    "    # We need to check if we already have a session saved or need to log to Facebook\n",
    "    def new_session(self):\n",
    "        if not os.path.exists(self.cookies_path):\n",
    "            return True\n",
    "\n",
    "        f = open(self.cookies_path, 'rb')\n",
    "        cookies = pickle.load(f)\n",
    "        self.session.cookies = cookies\n",
    "        return False\n",
    "\n",
    "    # Utility function to make the requests and convert to soup object if necessary\n",
    "    def make_request(self, url, method='GET', data=None, is_soup=True):\n",
    "        if len(url) == 0:\n",
    "            raise Exception(f'Empty Url')\n",
    "\n",
    "        if method == 'GET':\n",
    "            resp = self.session.get(url, headers=self.headers)\n",
    "        elif method == 'POST':\n",
    "            resp = self.session.post(url, headers=self.headers, data=data)\n",
    "        else:\n",
    "            raise Exception(f'Method [{method}] Not Supported')\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f'Error [{resp.status_code}] > {url}')\n",
    "\n",
    "        if is_soup:\n",
    "            return BeautifulSoup(resp.text, 'lxml')\n",
    "        return resp\n",
    "\n",
    "    # The first time we login\n",
    "    def login(self):\n",
    "        # Get the content of HTML of mobile Login Facebook page\n",
    "        url_home = \"https://m.facebook.com/\"\n",
    "        soup = self.make_request(url_home)\n",
    "        if soup is None:\n",
    "            raise Exception(\"Couldn't load the Login Page\")\n",
    "\n",
    "        # Here we need to extract this tokens from the Login Page\n",
    "        lsd = soup.find(\"input\", {\"name\": \"lsd\"}).get(\"value\")\n",
    "        jazoest = soup.find(\"input\", {\"name\": \"jazoest\"}).get(\"value\")\n",
    "        m_ts = soup.find(\"input\", {\"name\": \"m_ts\"}).get(\"value\")\n",
    "        li = soup.find(\"input\", {\"name\": \"li\"}).get(\"value\")\n",
    "        try_number = soup.find(\"input\", {\"name\": \"try_number\"}).get(\"value\")\n",
    "        unrecognized_tries = soup.find(\"input\", {\"name\": \"unrecognized_tries\"}).get(\"value\")\n",
    "\n",
    "        # This is the url to send the login params to Facebook\n",
    "        url_login = \"https://m.facebook.com/login/device-based/regular/login/?refsrc=https%3A%2F%2Fm.facebook.com%2F&lwv=100&refid=8\"\n",
    "        payload = {\n",
    "            \"lsd\": lsd,\n",
    "            \"jazoest\": jazoest,\n",
    "            \"m_ts\": m_ts,\n",
    "            \"li\": li,\n",
    "            \"try_number\": try_number,\n",
    "            \"unrecognized_tries\": unrecognized_tries,\n",
    "            \"email\": self.email,\n",
    "            \"pass\": self.password,\n",
    "            \"login\": \"Iniciar sesiÃ³n\",\n",
    "            \"prefill_contact_point\": \"\",\n",
    "            \"prefill_source\": \"\",\n",
    "            \"prefill_type\": \"\",\n",
    "            \"first_prefill_source\": \"\",\n",
    "            \"first_prefill_type\": \"\",\n",
    "            \"had_cp_prefilled\": \"false\",\n",
    "            \"had_password_prefilled\": \"false\",\n",
    "            \"is_smart_lock\": \"false\",\n",
    "            \"_fb_noscript\": \"true\"\n",
    "        }\n",
    "        soup = self.make_request(url_login, method='POST', data=payload, is_soup=True)\n",
    "        if soup is None:\n",
    "            raise Exception(f\"The login request couldn't be made: {url_login}\")\n",
    "\n",
    "        redirect = soup.select_one('a')\n",
    "        if not redirect:\n",
    "            raise Exception(\"Please log in desktop/mobile Facebook and change your password\")\n",
    "\n",
    "        url_redirect = redirect.get('href', '')\n",
    "        resp = self.make_request(url_redirect)\n",
    "        if resp is None:\n",
    "            raise Exception(f\"The login request couldn't be made: {url_redirect}\")\n",
    "\n",
    "        # Finally we get the cookies from the session and save it in a file for future usage\n",
    "        cookies = self.session.cookies\n",
    "        f = open(self.cookies_path, 'wb')\n",
    "        pickle.dump(cookies, f)\n",
    "\n",
    "        return {'code': 200}\n",
    "\n",
    "    # Scrap a list of profiles\n",
    "    def get_posts_from_list(self, profiles):\n",
    "        data = []\n",
    "        n = len(profiles)\n",
    "\n",
    "        for idx in range(n):\n",
    "            profile = profiles[idx]\n",
    "            print(f'{idx + 1}/{n}. {profile}')\n",
    "\n",
    "            posts = self.get_posts_from_profile(profile)\n",
    "            data.append(posts)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # This is the extraction point!\n",
    "    def get_posts_from_profile(self, url_profile):\n",
    "        # Prepare the Url to point to the posts feed\n",
    "        if \"www.\" in url_profile: url_profile = url_profile.replace('www.', 'm.')\n",
    "        if 'v=timeline' not in url_profile:\n",
    "            if '?' in url_profile:\n",
    "                url_profile = f'{url_profile}&v=timeline'\n",
    "            else:\n",
    "                url_profile = f'{url_profile}?v=timeline'\n",
    "\n",
    "        is_group = '/groups/' in url_profile\n",
    "\n",
    "        # Make a simple GET request\n",
    "        soup = self.make_request(url_profile)\n",
    "        if soup is None:\n",
    "            print(f\"Couldn't load the Page: {url_profile}\")\n",
    "            return []\n",
    "\n",
    "        # Now the extraction...\n",
    "        css_profile = '.storyStream > div'  # Select the posts from a user profile\n",
    "        css_page = '#recent > div > div > div'  # Select the posts from a Facebook page\n",
    "        css_group = '#m_group_stories_container > div > div'  # Select the posts from a Facebook group\n",
    "        raw_data = soup.select(f'{css_profile} , {css_page} , {css_group}')  # Now join and scrape it\n",
    "        posts = []\n",
    "        for item in raw_data:  # Now, for every post...\n",
    "            published = item.select_one('abbr')  # Get the formatted datetime of published\n",
    "            description = item.select('p')  # Get list of all p tag, they compose the description\n",
    "            images = item.select('a > img')  # Get list of all images\n",
    "            _external_links = item.select('p a')  # Get list of any link in the description, this are external links\n",
    "            post_url = item.find('a', text=self.post_url_text)  # Get the url to point this post.\n",
    "            like_url = item.find('a', text='Like')  # Get the Like url.\n",
    "\n",
    "            # Clean the publish date\n",
    "            if published is not None:\n",
    "                published = published.get_text()\n",
    "            else:\n",
    "                published = ''\n",
    "\n",
    "            # Join all the text in p tags, else set empty string\n",
    "            if len(description) > 0:\n",
    "                description = '\\n'.join([d.get_text() for d in description])\n",
    "            else:\n",
    "                description = ''\n",
    "\n",
    "            # Get all the images links\n",
    "            images = [image.get('src', '') for image in images]\n",
    "\n",
    "            # Clean the post link\n",
    "            if post_url is not None:\n",
    "                post_url = post_url.get('href', '')\n",
    "                if len(post_url) > 0:\n",
    "                    post_url = f'https://www.facebook.com{post_url}'\n",
    "                    p_url = urlparse(post_url)\n",
    "                    qs = parse_qs(p_url.query)\n",
    "                    if not is_group:\n",
    "                        post_url = f'{p_url.scheme}://{p_url.hostname}{p_url.path}?story_fbid={qs[\"story_fbid\"][0]}&id={qs[\"id\"][0]}'\n",
    "                    else:\n",
    "                        post_url = f'{p_url.scheme}://{p_url.hostname}{p_url.path}/permalink/{qs[\"id\"][0]}/'\n",
    "            else:\n",
    "                post_url = ''\n",
    "\n",
    "            # Clean the Like link\n",
    "            if like_url is not None:\n",
    "                like_url = like_url.get('href', '')\n",
    "                if len(like_url) > 0:\n",
    "                    like_url = f'https://m.facebook.com{like_url}'\n",
    "            else:\n",
    "                like_url = ''\n",
    "\n",
    "            # Get list of external links in post description, if any inside\n",
    "            external_links = []\n",
    "            for link in _external_links:\n",
    "                link = link.get('href', '')\n",
    "                try:\n",
    "                    a = link.index(\"u=\") + 2\n",
    "                    z = link.index(\"&h=\")\n",
    "                    link = unquote(link[a:z])\n",
    "                    link = link.split(\"?fbclid=\")[0]\n",
    "                    external_links.append(link)\n",
    "                except ValueError as e:\n",
    "                    continue\n",
    "            post = {'published': published, 'description': description, 'images': images,\n",
    "                    'post_url': post_url, 'external_links': external_links, 'like_url': like_url}\n",
    "            posts.append(post)\n",
    "            self.posts.append(post)\n",
    "        return posts\n",
    "\n",
    "    def posts_to_csv(self, filename):\n",
    "        if filename[:-4] != '.csv':\n",
    "            filename = f'{filename}.csv'\n",
    "\n",
    "        df = pd.DataFrame(self.posts)\n",
    "        df.to_csv(filename)\n",
    "\n",
    "    def posts_to_excel(self, filename):\n",
    "        if filename[:-5] != '.xlsx':\n",
    "            filename = f'{filename}.xlsx'\n",
    "\n",
    "        df = pd.DataFrame(self.posts)\n",
    "        df.to_excel(filename)\n",
    "\n",
    "    def posts_to_json(self, filename):\n",
    "        if filename[:-5] != '.json':\n",
    "            filename = f'{filename}.json'\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('[')\n",
    "            for entry in self.posts:\n",
    "                json.dump(entry, f)\n",
    "                f.write(',\\n')\n",
    "            f.write(']')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
